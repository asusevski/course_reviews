{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3019f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c996c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_courses = pd.read_json('./data/processed_data/course_data_clean.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bda9b",
   "metadata": {},
   "source": [
    "### Here's what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3208e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_code</th>\n",
       "      <th>course_title</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>useful</th>\n",
       "      <th>easy</th>\n",
       "      <th>liked</th>\n",
       "      <th>reviews</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>num_reviews_with_rating</th>\n",
       "      <th>good_course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS 115</td>\n",
       "      <td>Introduction to Computer Science 1</td>\n",
       "      <td>2111</td>\n",
       "      <td>21%</td>\n",
       "      <td>10%</td>\n",
       "      <td>23%</td>\n",
       "      <td>[{'review_text': 'go to office hours and pract...</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MATH 135</td>\n",
       "      <td>Algebra for Honours Mathematics</td>\n",
       "      <td>1186</td>\n",
       "      <td>84%</td>\n",
       "      <td>41%</td>\n",
       "      <td>78%</td>\n",
       "      <td>[{'review_text': 'Welcome to Waterloo Math.', ...</td>\n",
       "      <td>253</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ECON 101</td>\n",
       "      <td>Introduction to Microeconomics</td>\n",
       "      <td>1143</td>\n",
       "      <td>64%</td>\n",
       "      <td>70%</td>\n",
       "      <td>45%</td>\n",
       "      <td>[{'review_text': 'Took it online in W21 during...</td>\n",
       "      <td>214</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSYCH 101</td>\n",
       "      <td>Introductory Psychology</td>\n",
       "      <td>899</td>\n",
       "      <td>73%</td>\n",
       "      <td>67%</td>\n",
       "      <td>79%</td>\n",
       "      <td>[{'review_text': 'Really easy, the course was ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MATH 137</td>\n",
       "      <td>Calculus 1 for Honours Mathematics</td>\n",
       "      <td>780</td>\n",
       "      <td>86%</td>\n",
       "      <td>56%</td>\n",
       "      <td>69%</td>\n",
       "      <td>[{'review_text': 'and then isaac newton said \"...</td>\n",
       "      <td>171</td>\n",
       "      <td>167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8474</th>\n",
       "      <td>BE 680</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8475</th>\n",
       "      <td>KIN 658</td>\n",
       "      <td>Physical Activity and Cognition</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>ERS 620</td>\n",
       "      <td>Skills Identification and Career Development</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8477</th>\n",
       "      <td>KIN 659</td>\n",
       "      <td>Wearable Technology</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>COMMST 101</td>\n",
       "      <td>Theories of Communication</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8479 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     course_code                                  course_title  num_ratings  \\\n",
       "0         CS 115            Introduction to Computer Science 1         2111   \n",
       "1       MATH 135               Algebra for Honours Mathematics         1186   \n",
       "2       ECON 101                Introduction to Microeconomics         1143   \n",
       "3      PSYCH 101                       Introductory Psychology          899   \n",
       "4       MATH 137            Calculus 1 for Honours Mathematics          780   \n",
       "...          ...                                           ...          ...   \n",
       "8474      BE 680                                    Consulting            0   \n",
       "8475     KIN 658               Physical Activity and Cognition            0   \n",
       "8476     ERS 620  Skills Identification and Career Development            0   \n",
       "8477     KIN 659                           Wearable Technology            0   \n",
       "8478  COMMST 101                     Theories of Communication            0   \n",
       "\n",
       "     useful easy liked                                            reviews  \\\n",
       "0       21%  10%   23%  [{'review_text': 'go to office hours and pract...   \n",
       "1       84%  41%   78%  [{'review_text': 'Welcome to Waterloo Math.', ...   \n",
       "2       64%  70%   45%  [{'review_text': 'Took it online in W21 during...   \n",
       "3       73%  67%   79%  [{'review_text': 'Really easy, the course was ...   \n",
       "4       86%  56%   69%  [{'review_text': 'and then isaac newton said \"...   \n",
       "...     ...  ...   ...                                                ...   \n",
       "8474    N/A  N/A   N/A                                                 []   \n",
       "8475    N/A  N/A   N/A                                                 []   \n",
       "8476    N/A  N/A   N/A                                                 []   \n",
       "8477    N/A  N/A   N/A                                                 []   \n",
       "8478    N/A  N/A   N/A                                                 []   \n",
       "\n",
       "      num_reviews  num_reviews_with_rating  good_course  \n",
       "0              86                       83            0  \n",
       "1             253                      250            1  \n",
       "2             214                      210            0  \n",
       "3               8                        8            1  \n",
       "4             171                      167            1  \n",
       "...           ...                      ...          ...  \n",
       "8474            0                        0            0  \n",
       "8475            0                        0            0  \n",
       "8476            0                        0            0  \n",
       "8477            0                        0            0  \n",
       "8478            0                        0            0  \n",
       "\n",
       "[8479 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8dbbba",
   "metadata": {},
   "source": [
    "### And here's an example of what a review looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5760d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_text': 'go to office hours and practice',\n",
       " 'course_rating': 'liked course',\n",
       " 'course_rating_int': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_courses['reviews'].values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd5567",
   "metadata": {},
   "source": [
    "Quick preprocessing to extract reviews and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e1974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = {'course_code': [], 'review': [], 'label': []}\n",
    "for course in df_courses[['course_code', 'reviews']].values:\n",
    "    course_code = course[0]\n",
    "    reviews = course[1]\n",
    "    retval = []\n",
    "    for review in reviews:\n",
    "        review_text = review['review_text']\n",
    "        label = review['course_rating_int']\n",
    "        if label == 0 or label == 1:\n",
    "            df_reviews['course_code'].append(course_code)\n",
    "            df_reviews['review'].append(review_text)\n",
    "            df_reviews['label'].append(label)\n",
    "        \n",
    "df_reviews = pd.DataFrame(df_reviews).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f99208af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(df_reviews['label'].values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(df_reviews['review'].values), y, test_size=0.33, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec64d2",
   "metadata": {},
   "source": [
    "## HF Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d780dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "2022-09-21 02:06:46.727236: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-21 02:06:46.852423: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-21 02:06:47.380098: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-09-21 02:06:47.380183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-09-21 02:06:47.380194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cbe0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': Dataset.from_dict({'label':y_train,'text':X_train}),\n",
    "    'test': Dataset.from_dict({'label':y_test,'text':X_test})\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ebbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3956afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(texts):\n",
    "    return tokenizer(texts[\"text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0987e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 31.48ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 36.71ba/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6250f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"bad course\", 1: \"good course\"}\n",
    "label2id = {\"bad course\": 0, \"good course\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74dee662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45938e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbee7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 42\n",
    "logging_steps = len(X_train) // batch_size\n",
    "output_dir = \"hf_trainer\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "     num_train_epochs=5,\n",
    "     learning_rate=2e-5,\n",
    "     per_device_train_batch_size=batch_size,\n",
    "     per_device_eval_batch_size=batch_size,\n",
    "     weight_decay=0.01,\n",
    "     evaluation_strategy=\"epoch\",\n",
    "     logging_steps=logging_steps,\n",
    "     #p16=True,\n",
    "     push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9cf802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c369b2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9792\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 42\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 84\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 585\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 03:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401144</td>\n",
       "      <td>0.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.817709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.417230</td>\n",
       "      <td>0.819622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.429989</td>\n",
       "      <td>0.820278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>0.820929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4824\n",
      "  Batch size = 84\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4824\n",
      "  Batch size = 84\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4824\n",
      "  Batch size = 84\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4824\n",
      "  Batch size = 84\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to hf_trainer/checkpoint-500\n",
      "Configuration saved in hf_trainer/checkpoint-500/config.json\n",
      "Model weights saved in hf_trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in hf_trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in hf_trainer/checkpoint-500/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4824\n",
      "  Batch size = 84\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=585, training_loss=0.33247195838863014, metrics={'train_runtime': 226.3056, 'train_samples_per_second': 216.345, 'train_steps_per_second': 2.585, 'total_flos': 3637978041828528.0, 'train_loss': 0.33247195838863014, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a163b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4824\n",
      "  Batch size = 84\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4539097547531128,\n",
       " 'eval_f1': 0.8209291020117502,\n",
       " 'eval_runtime': 7.4773,\n",
       " 'eval_samples_per_second': 645.154,\n",
       " 'eval_steps_per_second': 7.757,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8058d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model/distilbert_course_reviews_01\n",
      "Configuration saved in ./saved_model/distilbert_course_reviews_01/config.json\n",
      "Model weights saved in ./saved_model/distilbert_course_reviews_01/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_model/distilbert_course_reviews_01/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_model/distilbert_course_reviews_01/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model_dir = './saved_model/'\n",
    "trainer.save_model(model_dir + 'distilbert_course_reviews_01')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
